{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # data visualization\n",
        "import seaborn as sns # statistical data visualization\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "# mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VMyGnEi-iEg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"/content/AUS_Weather.csv\"\n",
        "\n",
        "df = pd.read_csv(data)\n",
        "\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "o9HrUXbsiEjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns : \", df.columns)\n",
        "print(\"\\n\")\n",
        "print(\"Rows: \", df.shape[0])\n",
        "print(\"\\n\")\n",
        "print(\"Columns: \", df.shape[1])\n",
        "print(\"\\n\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "hNGhv-FhiElt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the percentage of missing values\n",
        "missing_values = df.isnull().sum() / len(df) * 100\n",
        "\n",
        "# create a dataframe for better visualization\n",
        "missing_values_df = pd.DataFrame({'column': missing_values.index,\n",
        "                                  'percentage': missing_values.values})\n",
        "\n",
        "# sort the dataframe by percentage in descending order\n",
        "missing_values_df = missing_values_df.sort_values(by='percentage', ascending=False)\n",
        "\n",
        "# display the dataframe with horizontal bars using styling\n",
        "display(missing_values_df.style.background_gradient(subset=['percentage'], cmap='Blues'))"
      ],
      "metadata": {
        "id": "GGGF-jEIiEoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def column_uniques_and_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with the number of unique non-null values and\n",
        "    the count of NaN values for each column in the input DataFrame.\n",
        "    \"\"\"\n",
        "    # Count unique non-null values per column\n",
        "    unique_counts = df.nunique(dropna=False)\n",
        "\n",
        "    # Count missing values per column\n",
        "    missing_counts = df.isnull().sum()\n",
        "\n",
        "    # Combine into a summary DataFrame\n",
        "    summary = pd.DataFrame({\n",
        "        'unique_values': unique_counts,\n",
        "        'missing_values': missing_counts\n",
        "    })\n",
        "    summary.index.name = 'column'\n",
        "\n",
        "    # Print or return the summary for inspection\n",
        "    return summary\n",
        "\n",
        "# Example usage with your dataset:\n",
        "data_path = '/content/AUS_Weather.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "summary_df = column_uniques_and_missing(df)\n",
        "summary_df"
      ],
      "metadata": {
        "id": "9m7LIP9SlhQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total NaN values across the entire DataFrame\n",
        "total_nan_count = df.isnull().sum().sum() # .isnull().sum() gives null per column\n",
        "# .isnull().sum().sum() gives absolute total across dataframe\n",
        "print(f\"\\nTotal NaN values in the dataset: {total_nan_count}\")"
      ],
      "metadata": {
        "id": "fqmLmBvYmVhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YcmGlln5mVkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seperating categorical and numerical variables\n",
        "# find categorical variables\n",
        "\n",
        "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
        "\n",
        "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
        "\n",
        "print('The categorical variables are :', categorical)"
      ],
      "metadata": {
        "id": "mItujG-sk16m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view the categorical variables\n",
        "\n",
        "df[categorical].head()"
      ],
      "metadata": {
        "id": "2PSSkNRvk19g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of categorical variables:\n",
        "\n",
        "There is a date variable. It is denoted by Date column.\n",
        "\n",
        "There are 6 categorical variables.\n",
        "\n",
        "These are given by Location, WindGustDir, WindDir9am, WindDir3pm, RainToday and RainTomorrow.\n",
        "\n",
        "There are two binary categorical variables - RainToday and RainTomorrow.\n",
        "\n",
        "RainTomorrow is the target variable."
      ],
      "metadata": {
        "id": "GkpcLkg-lNTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore problems within categorical variables:\n",
        "\n",
        "First, I will explore the categorical variables.\n",
        "\n",
        "Missing values in categorical variables"
      ],
      "metadata": {
        "id": "LSLp_4vblP0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in categorical variables\n",
        "\n",
        "df[categorical].isnull().sum()"
      ],
      "metadata": {
        "id": "PSVa-KFyk2AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are only 4 categorical variables in the dataset which contains missing values. These are WindGustDir, WindDir9am, WindDir3pm and RainToday.\n",
        "\n",
        "Frequency counts of categorical variables:\n",
        "\n",
        "Now, I will check the frequency counts of categorical variables."
      ],
      "metadata": {
        "id": "-aExMPmAm6wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view frequency of categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(df[var].value_counts())"
      ],
      "metadata": {
        "id": "qvTeDU0Ak2Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view frequency distribution of categorical variables (in percentage)\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(df[var].value_counts()/float(len(df)))"
      ],
      "metadata": {
        "id": "JQXL1l5vk2E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of labels: cardinality:\n",
        "\n",
        "The number of labels within a categorical variable is known as cardinality.\n",
        "\n",
        "A high number of labels within a variable is known as high cardinality.\n",
        "\n",
        "High cardinality may pose some serious problems in the machine learning model. So, I will check for high cardinality.\n",
        "\n",
        "\n",
        "### But what are these problems due to Cardinality ?\n",
        "\n",
        "Increased Memory Usage: Each unique category requires memory to store and process, and with high cardinality, this can lead to excessive memory consumption, especially with large datasets.\n",
        "\n",
        "\n",
        "Increased Training Time: Models need to process a larger number of features when dealing with high cardinality after techniques like one-hot encoding. This can significantly increase the training time.\n",
        "\n",
        "\n",
        "Increased Risk of Overfitting: High cardinality can lead to a large number of features, which can make the model too complex and cause it to overfit to the training data, performing poorly on unseen data.\n",
        "\n",
        "\n",
        "Sparse Data: One-hot encoding a high cardinality feature results in a large number of columns with mostly zero values, leading to sparse data. This can be challenging for some algorithms and might require specific handling.\n",
        "\n",
        "\n",
        "Reduced Interpretability: Models with a large number of features due to high cardinality can be harder to interpret and understand, making it difficult to explain the model's predictions.\n"
      ],
      "metadata": {
        "id": "E_QJqArKnGlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for cardinality in categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(var, ' contains ', len(df[var].unique()), ' labels')"
      ],
      "metadata": {
        "id": "AICdtkBtm-AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Firstly , we know that RainTomorrow is the Target label\n",
        "# so let us clean this first and then move to input features\n",
        "\n",
        "df['RainTomorrow'].unique()"
      ],
      "metadata": {
        "id": "tKTRfAHwm-Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainTomorrow'].value_counts()"
      ],
      "metadata": {
        "id": "FpCH3tAIk2Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_dist = df['RainTomorrow'].value_counts(dropna=False)\n",
        "\n",
        "print(count_dist)\n",
        "\n",
        "# NaN values are 3267 which needs to be removed."
      ],
      "metadata": {
        "id": "OesPJtPFpXyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['RainTomorrow'], inplace=True)"
      ],
      "metadata": {
        "id": "nfairJHbk2M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_dist = df['RainTomorrow'].value_counts(dropna=False)\n",
        "\n",
        "print(count_dist)\n",
        "\n",
        "# Nan values have been removed"
      ],
      "metadata": {
        "id": "q1D3nos_k2PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is a Date variable which needs to be preprocessed. I will do preprocessing in the following section.\n",
        "\n",
        "All the other variables contain relatively smaller number of variables.\n",
        "\n",
        "Feature Engineering of Date Variable :"
      ],
      "metadata": {
        "id": "POBnEoNZqAHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'].dtypes\n",
        "# this is object datatype, needs to converted to datetime"
      ],
      "metadata": {
        "id": "gqiOgbrLk2R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the dates, currently coded as strings, into datetime format\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "sH4_BFzUp-cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'].dtypes"
      ],
      "metadata": {
        "id": "ECGp0Bdvp-em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract year from date\n",
        "\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "df['Year'].head()"
      ],
      "metadata": {
        "id": "Y4gSCO5vp-hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract month from date\n",
        "\n",
        "df['Month'] = df['Date'].dt.month\n",
        "\n",
        "df['Month'].head()"
      ],
      "metadata": {
        "id": "psx5jAYcp-kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract day from date\n",
        "\n",
        "df['Day'] = df['Date'].dt.day\n",
        "\n",
        "df['Day'].head()"
      ],
      "metadata": {
        "id": "KkpZg6c8p-mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# again view the summary of dataset\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "m-OP0LRDp-pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that there are three additional columns created from Date variable.\n",
        "# Now, I will drop the original Date variable from the dataset.\n",
        "\n",
        "\n",
        "df.drop('Date', axis=1, inplace = True)"
      ],
      "metadata": {
        "id": "2mxLA-znp-rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preview the dataset again\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-B2CK6aTp-uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we can see that the Date variable has been removed from the dataset.\n",
        "\n",
        "# Explore Categorical Variables\n",
        "\n",
        "# Now, I will explore the categorical variables one by one.\n",
        "\n",
        "# find categorical variables\n",
        "\n",
        "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
        "\n",
        "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
        "\n",
        "print('The categorical variables are :', categorical)"
      ],
      "metadata": {
        "id": "HEkJABk9p-w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for missing values in categorical variables\n",
        "\n",
        "df[categorical].isnull().sum()\n"
      ],
      "metadata": {
        "id": "HHUyPIDMp-za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that WindGustDir, WindDir9am, WindDir3pm, RainToday variables contain missing values. I will explore these variables one by one."
      ],
      "metadata": {
        "id": "PQd5_ROvqZCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore Location variable\n",
        "\n",
        "# print number of labels in Location variable\n",
        "\n",
        "print('Location contains', len(df.Location.unique()), 'labels')"
      ],
      "metadata": {
        "id": "ikVVVFiyp-11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check labels in location variable\n",
        "\n",
        "df.Location.unique()"
      ],
      "metadata": {
        "id": "xeu9SAi2p-4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check frequency distribution of values in Location variable\n",
        "\n",
        "df.Location.value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "AOpl8seCp-6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do One Hot Encoding of Location variable\n",
        "# get k-1 dummy variables after One Hot Encoding\n",
        "# preview the dataset with head() method\n",
        "\n",
        "pd.get_dummies(df.Location, drop_first=True).head()"
      ],
      "metadata": {
        "id": "JhFOF6NUqcdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet pd.get_dummies(df.Location, drop_first=True).head() performs the following actions:\n",
        "\n",
        "pd.get_dummies(df.Location, ...): This is a pandas function that converts the categorical variable df.Location into dummy or indicator variables.\n",
        "\n",
        "For each unique location in the 'Location' column, it creates a new column. drop_first=True: This argument is used to drop the first category of the 'Location' variable.\n",
        "\n",
        "This is often done to avoid multicollinearity when using these dummy variables in a statistical model. By dropping the first category, the remaining categories can be interpreted in comparison to the dropped one.\n",
        "\n",
        ".head(): This method is called on the resulting DataFrame of dummy variables to display only the first 5 rows. This gives you a preview of what the one-hot encoded data looks like.\n",
        "\n",
        "In essence, this code is performing one-hot encoding on the 'Location' column, creating a new binary column for each location (except for one, which is dropped), and then showing you the beginning of this new set of columns."
      ],
      "metadata": {
        "id": "r4rKZFh2qvdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore WindGustDir variable\n",
        "\n",
        "# print number of labels in WindGustDir variable\n",
        "\n",
        "print('WindGustDir contains', len(df['WindGustDir'].unique()), 'labels')\n"
      ],
      "metadata": {
        "id": "ZyNHdpEKqcf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check labels in WindGustDir variable\n",
        "\n",
        "df['WindGustDir'].unique()"
      ],
      "metadata": {
        "id": "X5SBuZxSqciO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the above column too contains nan that needs to be removed\n",
        "df.dropna(subset=['WindGustDir'], inplace=True)"
      ],
      "metadata": {
        "id": "lVsNkVz1qckz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindGustDir'].isnull().sum()"
      ],
      "metadata": {
        "id": "PnAV7FL1qcnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do One Hot Encoding of WindGustDir variable\n",
        "# get k-1 dummy variables after One Hot Encoding\n",
        "# also add an additional dummy variable to indicate there was missing data\n",
        "# preview the dataset with head() method\n",
        "\n",
        "pd.get_dummies(df.WindGustDir, drop_first=True, dummy_na=True).head()"
      ],
      "metadata": {
        "id": "je4EToQfqcpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum the number of 1s per boolean variable over the rows of the dataset\n",
        "# it will tell us how many observations we have for each category\n",
        "\n",
        "pd.get_dummies(df.WindGustDir, drop_first=True, dummy_na=True).sum(axis=0)"
      ],
      "metadata": {
        "id": "J3GmdwJ9qcsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that there are 9330 missing values in WindGustDir variable.\n",
        "# Explore WindDir9am variable\n",
        "# print number of labels in WindDir9am variable\n",
        "\n",
        "print('WindDir9am contains', len(df['WindDir9am'].unique()), 'labels')"
      ],
      "metadata": {
        "id": "pTuGWzAhqcuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check labels in WindDir9am variable\n",
        "\n",
        "df['WindDir9am'].unique()"
      ],
      "metadata": {
        "id": "YNTpEwR8qcxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the above column too contains nan that needs to be removed\n",
        "df.dropna(subset=['WindDir9am'], inplace=True)"
      ],
      "metadata": {
        "id": "KRzXuHjlp-9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir9am'].isnull().sum()"
      ],
      "metadata": {
        "id": "_3jzeXkap_AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do One Hot Encoding of WindDir9am variable\n",
        "# get k-1 dummy variables after One Hot Encoding\n",
        "# also add an additional dummy variable to indicate there was missing data\n",
        "# preview the dataset with head() method\n",
        "\n",
        "pd.get_dummies(df.WindDir9am, drop_first=True, dummy_na=True).head()"
      ],
      "metadata": {
        "id": "JhYpZxBPp_FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum the number of 1s per boolean variable over the rows of the dataset\n",
        "# it will tell us how many observations we have for each category\n",
        "\n",
        "pd.get_dummies(df.WindDir9am, drop_first=True, dummy_na=True).sum(axis=0)"
      ],
      "metadata": {
        "id": "nXqi5z6Ircca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that there are 10013 missing values in the WindDir9am variable.\n",
        "# Explore WindDir3pm variable\n",
        "\n",
        "# print number of labels in WindDir3pm variable\n",
        "\n",
        "print('WindDir3pm contains', len(df['WindDir3pm'].unique()), 'labels')"
      ],
      "metadata": {
        "id": "xI313TK1rce_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the above column too contains nan that needs to be removed\n",
        "df.dropna(subset=['WindDir3pm'], inplace=True)"
      ],
      "metadata": {
        "id": "jI6dEKI3riKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir3pm'].isnull().sum()"
      ],
      "metadata": {
        "id": "pQN-PCotriMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do One Hot Encoding of WindDir3pm variable\n",
        "# get k-1 dummy variables after One Hot Encoding\n",
        "# also add an additional dummy variable to indicate there was missing data\n",
        "# preview the dataset with head() method\n",
        "\n",
        "pd.get_dummies(df.WindDir3pm, drop_first=True, dummy_na=True).head()"
      ],
      "metadata": {
        "id": "T5Hw1ptkriPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum the number of 1s per boolean variable over the rows of the dataset\n",
        "# it will tell us how many observations we have for each category\n",
        "\n",
        "pd.get_dummies(df.WindDir3pm, drop_first=True, dummy_na=True).sum(axis=0)"
      ],
      "metadata": {
        "id": "pECJkBekrchO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are 3778 missing values in the WindDir3pm variable.\n",
        "# Explore RainToday variable\n",
        "\n",
        "# print number of labels in RainToday variable\n",
        "\n",
        "print('RainToday contains', len(df['RainToday'].unique()), 'labels')"
      ],
      "metadata": {
        "id": "h1PnXGWnrcjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check labels in WindGustDir variable\n",
        "\n",
        "df['RainToday'].unique()"
      ],
      "metadata": {
        "id": "Gi93EBKRr7-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the above column too contains nan that needs to be removed\n",
        "df.dropna(subset=['RainToday'], inplace=True)\n",
        "\n",
        "df['RainToday'].isnull().sum()"
      ],
      "metadata": {
        "id": "I3r8jMEgr8BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Calculate value counts\n",
        "rain_today_counts = df['RainToday'].value_counts()\n",
        "\n",
        "# Create a pie chart using Plotly Express\n",
        "fig = px.pie(\n",
        "    rain_today_counts,\n",
        "    values=rain_today_counts.values,\n",
        "    names=rain_today_counts.index,\n",
        "    title='Distribution of RainToday'\n",
        ")\n",
        "\n",
        "# Display the chart\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "XUM9S6rsr8Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do One Hot Encoding of RainToday variable\n",
        "# get k-1 dummy variables after One Hot Encoding\n",
        "# also add an additional dummy variable to indicate there was missing data\n",
        "# preview the dataset with head() method\n",
        "\n",
        "pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True).head()"
      ],
      "metadata": {
        "id": "loJsX53asLuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum the number of 1s per boolean variable over the rows of the dataset\n",
        "# it will tell us how many observations we have for each category\n",
        "\n",
        "pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True).sum(axis=0)"
      ],
      "metadata": {
        "id": "2fjuHK2vsLwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True).sum(axis=0).\n",
        "\n",
        "pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True): This part performs one-hot encoding on the 'RainToday' column.\n",
        "\n",
        "df.RainToday: Selects the 'RainToday' column from your DataFrame. drop_first=True: This creates dummy variables for each unique value in 'RainToday' but drops the first category to avoid multicollinearity.\n",
        "\n",
        "dummy_na=True: This is important! It creates an additional dummy variable specifically for the missing (NaN) values in the 'RainToday' column.\n",
        "\n",
        ".sum(axis=0): This part takes the resulting DataFrame of dummy variables and calculates the sum of values along each column (axis=0). Since the dummy variables are binary (0 or 1), summing along the column essentially counts the number of occurrences of each category, including the missing values.\n",
        "\n",
        "In summary, this code snippet one-hot encodes the 'RainToday' column, including a separate category for missing values, and then counts how many times each category (including missing) appears in the dataset. This gives you a quick way to see the distribution of values in the 'RainToday' column, including the count of missing entries after one-hot encoding."
      ],
      "metadata": {
        "id": "Q8c7MXyhsYM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
        "\n",
        "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
        "\n",
        "print('The numerical variables are :', numerical)"
      ],
      "metadata": {
        "id": "x_03GHObsVaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view the numerical variables\n",
        "\n",
        "df[numerical].head()"
      ],
      "metadata": {
        "id": "J3OJPGCdsVcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of numerical variables:\n",
        "\n",
        "There are 16 numerical variables. These are given by MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Cloud9am, Cloud3pm, Temp9am and Temp3pm. All of the numerical variables are of continuous type.\n",
        "\n"
      ],
      "metadata": {
        "id": "t0MutVtdshib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore problems within numerical variables\n",
        "\n",
        "Now, I will explore the numerical variables.\n",
        "\n",
        "Missing values in numerical variables"
      ],
      "metadata": {
        "id": "GInrSgrLsl_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in numerical variables\n",
        "\n",
        "df[numerical].isnull().sum()\n"
      ],
      "metadata": {
        "id": "f_ZKWwDEsiYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in numerical variables\n",
        "missing_numerical = df[numerical].isnull().sum()\n",
        "\n",
        "# calculate percentage of missing values\n",
        "missing_numerical_percentage = (missing_numerical / len(df) * 100).sort_values(ascending=False)\n",
        "\n",
        "# create a dataframe for visualization\n",
        "missing_numerical_df = pd.DataFrame({\n",
        "    'Missing Count': missing_numerical[missing_numerical_percentage.index],\n",
        "    'Percentage': missing_numerical_percentage\n",
        "})\n",
        "\n",
        "# display with horizontal bars using styling\n",
        "display(missing_numerical_df.style.background_gradient(subset=['Percentage'], cmap='Blues'))"
      ],
      "metadata": {
        "id": "cpXUXqdCsiba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that all the 16 numerical variables contain missing values.\n",
        "# Outliers in numerical variables\n",
        "# view summary statistics in numerical variables\n",
        "\n",
        "df[numerical].describe()\n"
      ],
      "metadata": {
        "id": "EJ6KqJqssVg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Values: The count row shows that many numerical columns have a significant number of missing values, as we saw in the previous visualizations.\n",
        "\n",
        "Columns like Sunshine, Evaporation, Cloud9am, and Cloud3pm have considerably fewer counts than the total number of rows (145460), indicating a high percentage of missing data.\n",
        "\n",
        "Range of Values: The min and max rows give you the range of values for each numerical variable. This can help identify potential outliers or data entry errors. For example, Rainfall has a maximum value of 371.0, which seems quite high compared to the 75th percentile of 0.8, suggesting the presence of outliers.\n",
        "\n",
        "Distribution (Mean vs. Median): Comparing the mean and 50% (median) values can give you an idea of the distribution of the data. If the mean and median are significantly different, it might indicate a skewed distribution.\n",
        "\n",
        "For instance, the mean of Rainfall (2.36) is much higher than its median (0.0), confirming a heavily skewed distribution with many days having no rain and a few days with very high rainfall.\n",
        "\n",
        "Variability (Standard Deviation): The std row shows the standard deviation, which measures the spread or variability of the data. A higher standard deviation indicates greater variability in the values of a variable."
      ],
      "metadata": {
        "id": "vsEGGj4asxUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On closer inspection, we can see that the Rainfall, Evaporation, WindSpeed9am and WindSpeed3pm columns may contain outliers.\n",
        "\n",
        "I will draw boxplots to visualise outliers in the above variables."
      ],
      "metadata": {
        "id": "2LHR_-A9sy9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# draw boxplots to visualize outliers\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "fig = df.boxplot(column='Rainfall')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('Rainfall')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "fig = df.boxplot(column='Evaporation')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('Evaporation')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "fig = df.boxplot(column='WindSpeed9am')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('WindSpeed9am')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "fig = df.boxplot(column='WindSpeed3pm')\n",
        "fig.set_title('')\n",
        "fig.set_ylabel('WindSpeed3pm')"
      ],
      "metadata": {
        "id": "ntJvDn-DrcmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rainfall: The boxplot for Rainfall shows a large number of points far above the upper whisker. This indicates a significant number of outliers with high rainfall values. The majority of the data is concentrated near zero, as expected for rainfall data.\n",
        "\n",
        "Evaporation: The boxplot for Evaporation also shows several points above the upper whisker, indicating outliers with unusually high evaporation rates. The distribution appears less skewed than Rainfall, but outliers are present.\n",
        "\n",
        "WindSpeed9am and WindSpeed3pm: Both wind speed variables show outliers above the upper whisker. This suggests there were instances of unusually high wind speeds recorded at both 9 am and 3 pm. The spread of the main body of the data (within the box and whiskers) seems more symmetrical compared to Rainfall and Evaporation.\n",
        "\n",
        "In summary, all four variables show the presence of outliers, with Rainfall exhibiting the most extreme ones. These outliers should be considered when preparing the data for modeling, as they can sometimes disproportionately influence certain algorithms."
      ],
      "metadata": {
        "id": "M43cAefgs23N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the distribution of variables:\n",
        "\n",
        "Now, I will plot the histograms to check distributions to find out if they are normal or skewed. If the variable follows normal distribution, then I will do Extreme Value Analysis otherwise if they are skewed, I will find IQR (Interquantile range)"
      ],
      "metadata": {
        "id": "Jn3GqvOms4vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot histogram to check distribution\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "fig = df.Rainfall.hist(bins=10)\n",
        "fig.set_xlabel('Rainfall')\n",
        "fig.set_ylabel('RainTomorrow')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "fig = df.Evaporation.hist(bins=10)\n",
        "fig.set_xlabel('Evaporation')\n",
        "fig.set_ylabel('RainTomorrow')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "fig = df.WindSpeed9am.hist(bins=10)\n",
        "fig.set_xlabel('WindSpeed9am')\n",
        "fig.set_ylabel('RainTomorrow')\n",
        "\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "fig = df.WindSpeed3pm.hist(bins=10)\n",
        "fig.set_xlabel('WindSpeed3pm')\n",
        "fig.set_ylabel('RainTomorrow')"
      ],
      "metadata": {
        "id": "RJVit8Sksv9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the histograms of 'Rainfall', 'Evaporation', 'WindSpeed9am', and 'WindSpeed3pm', here are some inferences:\n",
        "\n",
        "Rainfall: The histogram for Rainfall is highly skewed to the right. A very large number of observations have zero or very low rainfall, with a long tail extending towards higher rainfall values. This is consistent with the boxplot and the nature of rainfall data, where heavy rain events are less frequent but can have high values.\n",
        "\n",
        "Evaporation: The histogram for Evaporation is also skewed to the right, although less severely than Rainfall. Most values are concentrated at the lower end, with fewer occurrences of high evaporation rates.\n",
        "\n",
        "WindSpeed9am and WindSpeed3pm: The histograms for both WindSpeed9am and WindSpeed3pm appear to be more symmetrical and somewhat resemble a normal distribution, although there might be a slight skew to the right. The majority of the wind speed values are clustered around the lower to middle range, with fewer instances of very high wind speeds (which were identified as outliers in the boxplots).\n",
        "\n",
        "In summary, the histograms confirm the skewed nature of Rainfall and Evaporation, while the wind speed variables show distributions that are closer to normal but still exhibit some right skew. This information about the distribution of these numerical variables is important for choosing appropriate data preprocessing techniques and modeling algorithms."
      ],
      "metadata": {
        "id": "jAY3uiz8s85I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that all the four variables are skewed.\n",
        "# So, I will use interquantile range to find outliers.\n",
        "\n",
        "\n",
        "# find outliers for Rainfall variable\n",
        "\n",
        "IQR = df.Rainfall.quantile(0.75) - df.Rainfall.quantile(0.25)\n",
        "Lower_fence = df.Rainfall.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = df.Rainfall.quantile(0.75) + (IQR * 3)\n",
        "print('Rainfall outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
      ],
      "metadata": {
        "id": "iFGdLpwDswAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferences from the output:\n",
        "\n",
        "The output you received is: Rainfall outliers are values < -2.4000000000000004 or > 3.2.\n",
        "\n",
        "Lower Fence: The lower fence is calculated as approximately -2.4. Since rainfall cannot be negative, this lower fence isn't practically useful for identifying outliers in this context. It simply indicates that there are no outliers on the lower end of the distribution based on this method.\n",
        "\n",
        "Upper Fence: The upper fence is calculated as 3.2. This means any rainfall value greater than 3.2 mm is considered an outlier according to the IQR method with a multiplier of 3.\n",
        "\n",
        "This confirms what we saw in the boxplot – there are many data points above this value, indicating frequent occurrences of higher rainfall amounts that are statistically identified as outliers relative to the majority of the data.\n",
        "\n",
        "This method helps to quantify what values are considered extreme in the 'Rainfall' distribution."
      ],
      "metadata": {
        "id": "4NkIztIYtBVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rainfall, the minimum and maximum values are 0.0 and 371.0. So, the outliers are values > 3.2.\n",
        "\n",
        "# find outliers for Evaporation variable\n",
        "\n",
        "IQR = df.Evaporation.quantile(0.75) - df.Evaporation.quantile(0.25)\n",
        "Lower_fence = df.Evaporation.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = df.Evaporation.quantile(0.75) + (IQR * 3)\n",
        "print('Evaporation outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))\n"
      ],
      "metadata": {
        "id": "NJAff1QzswCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Evaporation, the minimum and maximum values are 0.0 and 145.0.\n",
        "# So, the outliers are values > 21.8.\n",
        "\n",
        "# find outliers for WindSpeed9am variable\n",
        "\n",
        "IQR = df.WindSpeed9am.quantile(0.75) - df.WindSpeed9am.quantile(0.25)\n",
        "Lower_fence = df.WindSpeed9am.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = df.WindSpeed9am.quantile(0.75) + (IQR * 3)\n",
        "print('WindSpeed9am outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
      ],
      "metadata": {
        "id": "3FYx9uONswE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WindSpeed9am outliers are values < -29.0 or > 55.0\n",
        "# For WindSpeed9am, the minimum and maximum values are 0.0 and 130.0.\n",
        "#  So, the outliers are values > 55.0.\n",
        "\n",
        "# find outliers for WindSpeed3pm variable\n",
        "\n",
        "IQR = df.WindSpeed3pm.quantile(0.75) - df.WindSpeed3pm.quantile(0.25)\n",
        "Lower_fence = df.WindSpeed3pm.quantile(0.25) - (IQR * 3)\n",
        "Upper_fence = df.WindSpeed3pm.quantile(0.75) + (IQR * 3)\n",
        "print('WindSpeed3pm outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))"
      ],
      "metadata": {
        "id": "3CVyzQvCswHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WindSpeed3pm outliers are values < -20.0 or > 57.0\n",
        "# For WindSpeed3pm, the minimum and maximum values are 0.0 and 87.0.\n",
        "# So, the outliers are values > 57.0."
      ],
      "metadata": {
        "id": "hDEVy-IRswJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare feature vector and target variable\n",
        "\n",
        "X = df.drop(['RainTomorrow'], axis=1)\n",
        "\n",
        "y = df['RainTomorrow']"
      ],
      "metadata": {
        "id": "nfXkJ35CswLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into separate training and test set\n",
        "\n",
        "# split X and y into training and testing sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
        "                                                    random_state = 0)"
      ],
      "metadata": {
        "id": "DD933JtXswOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X: This is your feature dataset (the independent variables).\n",
        "\n",
        "y: This is your target variable (the dependent variable you want to predict). test_size = 0.2: This argument specifies the proportion of the dataset that should be allocated to the testing set. In this case, 20% of the data will be used for testing, and the remaining 80% will be used for training.\n",
        "\n",
        "random_state = 0: This argument is used to set a seed for the random number generator. Setting a random_state ensures that the split is the same every time you run the code. This is important for reproducibility, as it allows you to get the same training and testing sets consistently."
      ],
      "metadata": {
        "id": "XjhhnCkrvKzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the shape of X_train and X_test\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "yH_Klq32swQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering:\n",
        "\n",
        "Feature Engineering is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables.\n",
        "\n",
        "First, I will display the categorical and numerical variables again separately."
      ],
      "metadata": {
        "id": "NoaChjvqvNi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check data types in X_train\n",
        "\n",
        "X_train.dtypes"
      ],
      "metadata": {
        "id": "Gpfd2UwkswSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display categorical variables\n",
        "\n",
        "categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n",
        "\n",
        "categorical"
      ],
      "metadata": {
        "id": "vIlhhIsVswU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display numerical variables\n",
        "\n",
        "numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n",
        "\n",
        "numerical"
      ],
      "metadata": {
        "id": "iynsXWlvswXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Engineering missing values in numerical variables\n",
        "\n",
        "# check missing values in numerical variables in X_train\n",
        "\n",
        "X_train[numerical].isnull().sum()"
      ],
      "metadata": {
        "id": "9pB-jtRrswZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in numerical variables in X_test\n",
        "\n",
        "X_test[numerical].isnull().sum()"
      ],
      "metadata": {
        "id": "o58q17javX_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print percentage of missing values in the numerical variables in training set\n",
        "\n",
        "for col in numerical:\n",
        "    if X_train[col].isnull().mean()>0:\n",
        "        print(col,\": \", round(X_train[col].isnull().mean(),4))"
      ],
      "metadata": {
        "id": "BmMkti1nvYB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for col in numerical:: This loop iterates through each column name stored in the numerical list. The numerical list contains the names of all the numerical columns in your DataFrame.\n",
        "\n",
        "if X_train[col].isnull().mean()>0:: Inside the loop, for each col (numerical column name), this condition checks if that column in the X_train DataFrame contains any missing values. X_train[col].isnull(): This creates a boolean Series (True/False) indicating whether each value in the column is missing (True) or not (False).\n",
        "\n",
        ".mean(): This calculates the mean of the boolean Series. Since True is treated as 1 and False as 0, the mean represents the proportion or percentage of missing values in the column.\n",
        "\n",
        "0: This checks if the mean (percentage of missing values) is greater than 0. The code inside the if block will only execute if the column has at least one missing value.\n",
        "\n",
        "print(col, round(X_train[col].isnull().mean(),4)): If the condition in the if statement is true (i.e., there are missing values in the column), this line prints the column name and the percentage of missing values.\n",
        "\n",
        "col: Prints the name of the numerical column. round(X_train[col].isnull().mean(),4): Calculates the mean of the boolean Series (percentage of missing values) again and rounds it to 4 decimal places for better readability.\n",
        "\n",
        "In essence, this code snippet efficiently identifies numerical columns with missing values in your training set and prints the percentage of missing data"
      ],
      "metadata": {
        "id": "cwH_7TiQvij8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assumption:\n",
        "\n",
        "I assume that the data are missing completely at random (MCAR).\n",
        "\n",
        "There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation.\n",
        "\n",
        "When there are outliers in the dataset, we should use median imputation. So, I will use median imputation because median imputation is robust to outliers.\n",
        "\n",
        "I will impute missing values with the appropriate statistical measures of the data, in this case median.\n",
        "\n",
        "Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only.\n",
        "\n",
        "This is to avoid overfitting."
      ],
      "metadata": {
        "id": "nAEDWDlyvkeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# impute missing values in X_train and X_test with respective column median in X_train\n",
        "\n",
        "for df1 in [X_train, X_test]:\n",
        "    for col in numerical:\n",
        "        col_median=X_train[col].median()\n",
        "        df1[col].fillna(col_median, inplace=True)\n",
        ""
      ],
      "metadata": {
        "id": "8CAL1AkTvcv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list of DataFrames to process\n",
        "\n",
        "for df1 in [X_train, X_test]:\n",
        "\n",
        "for df1 in [X_train, X_test]:: This is the outer loop. It's an elegant way to apply the same set of operations to multiple DataFrames without writing the code twice. In the first iteration, the variable df1 will refer to your X_train DataFrame. In the second iteration, df1 will refer to your X_test DataFrame.\n",
        "\n",
        "Loop through each numerical column name for col in numerical:\n",
        "\n",
        "for col in numerical:: This is the inner loop. It iterates through a predefined list called numerical, which presumably contains the names of all the columns you want to impute (e.g., ['age', 'salary', 'experience_years']). In each iteration of this inner loop, the variable col will hold one column name as a string (e.g., first 'age', then 'salary', etc.).\n",
        "\n",
        "Calculate the median ONLY from the training data col_median = X_train[col].median()\n",
        "\n",
        "col_median = X_train[col].median(): This is the most critical line. X_train[col]: It selects the current column (e.g., the 'age' column) but specifically and always from X_train. .median(): It calculates the median value of that column. The median is the middle value when the data is sorted, which makes it robust to outliers (unlike the mean). This calculated median is stored in the col_median variable. Notice that even when the outer loop is processing X_test, the median is still calculated from X_train.\n",
        "\n",
        "Fill missing values in the current DataFrame (df1) with the calculated median df1[col].fillna(col_median, inplace=True)\n",
        "\n",
        "df1[col].fillna(col_median, inplace=True): This line performs the actual imputation. df1[col]: Selects the current column (col) from the current DataFrame (df1). Remember, df1 could be X_train or X_test. .fillna(col_median): This is a pandas function that finds all the missing values (NaN) in the selected series and replaces them with the value of col_median. inplace=True: This argument modifies the DataFrame df1 directly in memory. Without it (inplace=False, the default), the operation would return a new series with the values filled, and you would need to assign it back, like this: df1[col] = df1[col].fillna(col_median). The inplace=True version is just a more concise way to write it."
      ],
      "metadata": {
        "id": "3aKm-YDZvrsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check again missing values in numerical variables in X_train\n",
        "\n",
        "X_train[numerical].isnull().sum()"
      ],
      "metadata": {
        "id": "Vm9dVTFWvcye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in numerical variables in X_test\n",
        "\n",
        "X_test[numerical].isnull().sum()"
      ],
      "metadata": {
        "id": "aeMVulFBvc0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ztQzwdK_vc3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we can see that there are no missing values in the numerical columns of training and test set.\n",
        "\n",
        "# Engineering missing values in categorical variables\n",
        "\n",
        "# print percentage of missing values in the categorical variables in training set\n",
        "\n",
        "X_train[categorical].isnull().sum()"
      ],
      "metadata": {
        "id": "Zi9D_X-Nvc6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print categorical variables with missing data\n",
        "\n",
        "for col in categorical:\n",
        "    if X_train[col].isnull().mean()>0:\n",
        "        print(col, (X_train[col].isnull().mean()))\n"
      ],
      "metadata": {
        "id": "EuYzq-HTvc-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# impute missing categorical variables with most frequent value\n",
        "\n",
        "for df2 in [X_train, X_test]:\n",
        "    df2['WindGustDir'].fillna(X_train['WindGustDir'].mode()[0], inplace=True)\n",
        "    df2['WindDir9am'].fillna(X_train['WindDir9am'].mode()[0], inplace=True)\n",
        "    df2['WindDir3pm'].fillna(X_train['WindDir3pm'].mode()[0], inplace=True)\n",
        "    df2['RainToday'].fillna(X_train['RainToday'].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "EJT_v-bYvdBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in categorical variables in X_train\n",
        "\n",
        "X_train[categorical].isnull().sum()"
      ],
      "metadata": {
        "id": "R-_qMF-2vdDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in categorical variables in X_test\n",
        "\n",
        "X_test[categorical].isnull().sum()"
      ],
      "metadata": {
        "id": "iSLTn4yGvdGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As a final check, I will check for missing values in X_train and X_test.\n",
        "\n",
        "# check missing values in X_train\n",
        "\n",
        "X_train.isnull().sum()"
      ],
      "metadata": {
        "id": "Rt60wD_wvdI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values in X_test\n",
        "\n",
        "X_test.isnull().sum()"
      ],
      "metadata": {
        "id": "4wZdG8JfvdL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that there are no missing values in X_train and X_test.\n",
        "\n",
        "# Engineering outliers in numerical variables\n",
        "# We have seen that the Rainfall, Evaporation, WindSpeed9am and WindSpeed3pm columns\n",
        "#  contain outliers. I will use top-coding approach to cap maximum values\n",
        "#  and remove outliers from the above variables."
      ],
      "metadata": {
        "id": "mphK9TOrvdOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_value(df3, variable, top):\n",
        "    return np.where(df3[variable]>top, top, df3[variable])\n",
        "\n",
        "for df3 in [X_train, X_test]:\n",
        "    df3['Rainfall'] = max_value(df3, 'Rainfall', 3.2)\n",
        "    df3['Evaporation'] = max_value(df3, 'Evaporation', 21.8)\n",
        "    df3['WindSpeed9am'] = max_value(df3, 'WindSpeed9am', 55)\n",
        "    df3['WindSpeed3pm'] = max_value(df3, 'WindSpeed3pm', 57)"
      ],
      "metadata": {
        "id": "jlEsMj9XvdRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.Rainfall.max(), X_test.Rainfall.max()"
      ],
      "metadata": {
        "id": "OGSqvMXGvdT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.Evaporation.max(), X_test.Evaporation.max()"
      ],
      "metadata": {
        "id": "uBazU6AJvdWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.WindSpeed9am.max(), X_test.WindSpeed9am.max()"
      ],
      "metadata": {
        "id": "WK1x3OwrvdZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.WindSpeed3pm.max(), X_test.WindSpeed3pm.max()"
      ],
      "metadata": {
        "id": "HqQVsuzWvdcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[numerical].describe()"
      ],
      "metadata": {
        "id": "vMqUe_AVvdfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now see that the outliers in Rainfall, Evaporation, WindSpeed9am and WindSpeed3pm columns are capped.\n",
        "\n",
        "# Encode categorical variables\n",
        "\n",
        "categorical"
      ],
      "metadata": {
        "id": "AJnZUc80vdih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[categorical].head()"
      ],
      "metadata": {
        "id": "DUBoT0mkvdln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode RainToday variable\n",
        "!pip install category_encoders\n",
        "\n",
        "import category_encoders as ce\n",
        "\n",
        "encoder = ce.BinaryEncoder(cols=['RainToday'])\n",
        "\n",
        "X_train = encoder.fit_transform(X_train)\n",
        "\n",
        "X_test = encoder.transform(X_test)"
      ],
      "metadata": {
        "id": "kLOWtF9xvdoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "U23jljavvdrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that two additional variables RainToday_0 and RainToday_1 are created from RainToday variable.\n",
        "\n",
        "Now, I will create the X_train training set."
      ],
      "metadata": {
        "id": "rN6Dmlb3zaJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.concat([X_train[numerical], X_train[['RainToday_0', 'RainToday_1']],\n",
        "                     pd.get_dummies(X_train.Location),\n",
        "                     pd.get_dummies(X_train.WindGustDir),\n",
        "                     pd.get_dummies(X_train.WindDir9am),\n",
        "                     pd.get_dummies(X_train.WindDir3pm)], axis=1)"
      ],
      "metadata": {
        "id": "ya5YpYcHvdue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "l0Iq_XiBzW81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarly, I will create the X_test testing set.\n",
        "\n",
        "X_test = pd.concat([X_test[numerical], X_test[['RainToday_0', 'RainToday_1']],\n",
        "                     pd.get_dummies(X_test.Location),\n",
        "                     pd.get_dummies(X_test.WindGustDir),\n",
        "                     pd.get_dummies(X_test.WindDir9am),\n",
        "                     pd.get_dummies(X_test.WindDir3pm)], axis=1)"
      ],
      "metadata": {
        "id": "jfdYW8EczW_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "id": "BR9yr3YfswcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called feature scaling. I will do it as follows."
      ],
      "metadata": {
        "id": "0RWhARjWzkZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling"
      ],
      "metadata": {
        "id": "sM5QhSw7zv0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe()"
      ],
      "metadata": {
        "id": "rnygUCmZze8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = X_train.columns"
      ],
      "metadata": {
        "id": "9oP4aXMrzfBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "mBJdSRV8zfFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame(X_train, columns=[cols])"
      ],
      "metadata": {
        "id": "CRqyxN5lz0zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "c5Ni10KLz04-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.DataFrame(X_test, columns=[cols])\n",
        "\n",
        "X_test.head()"
      ],
      "metadata": {
        "id": "EuVfd9Dcz07c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe()"
      ],
      "metadata": {
        "id": "O_sWkfTjzfJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We now have X_train dataset ready to be fed into the Logistic Regression classifier.\n",
        "# I will do it as follows."
      ],
      "metadata": {
        "id": "EiiKsEBsz6HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "4rDQ5efez-1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train a logistic regression model on the training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# instantiate the model\n",
        "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
        "\n",
        "\n",
        "# fit the model\n",
        "logreg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "D9ZYDCo7z6Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  Predict results\n",
        "y_pred_test = logreg.predict(X_test)\n",
        "\n",
        "y_pred_test\n"
      ],
      "metadata": {
        "id": "7iYM5YI8z_jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict_proba method:\n",
        "\n",
        "predict_proba method gives the probabilities for the target variable(0 and 1) in this case, in array form.\n",
        "\n",
        "0 is for probability of no rain and 1 is for probability of rain."
      ],
      "metadata": {
        "id": "lLwoHHux0Ttk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# probability of getting output as 0 - no rain\n",
        "\n",
        "logreg.predict_proba(X_test)[:,0]"
      ],
      "metadata": {
        "id": "CKQPPsMCz_nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probability of getting output as 1 - rain\n",
        "\n",
        "logreg.predict_proba(X_test)[:,1]"
      ],
      "metadata": {
        "id": "H4ZqztuUz_qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check accuracy score\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))"
      ],
      "metadata": {
        "id": "K-C-oRS9z6O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, y_test are the true class labels and y_pred_test are the predicted class labels in the test-set.\n",
        "\n",
        "Compare the train-set and test-set accuracy\n",
        "Now, I will compare the train-set and test-set accuracy to check for overfitting."
      ],
      "metadata": {
        "id": "E8VKFPuW0jL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = logreg.predict(X_train)\n",
        "\n",
        "y_pred_train"
      ],
      "metadata": {
        "id": "i3sa-S_A0caF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
      ],
      "metadata": {
        "id": "2PCcSbgD0cc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for overfitting and underfitting\n",
        "# print the scores on training and test set\n",
        "\n",
        "print('Training set score: {:.4f}'.format(logreg.score(X_train, y_train)))\n",
        "\n",
        "print('Test set score: {:.4f}'.format(logreg.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "6gFMwYp50lWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training-set accuracy score is 0.8476 while the test-set accuracy to be 0.8501. These two values are quite comparable. So, there is no question of overfitting.\n",
        "\n",
        "In Logistic Regression, we use default value of C = 1. It provides good performance with approximately 85% accuracy on both the training and the test set. But the model performance on both the training and test set are very comparable. It is likely the case of underfitting.\n",
        "\n",
        "I will increase C and fit a more flexible model."
      ],
      "metadata": {
        "id": "ExdBDv9_0roa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the Logsitic Regression model with C=100\n",
        "\n",
        "# instantiate the model\n",
        "logreg100 = LogisticRegression(C=100, solver='liblinear', random_state=0)\n",
        "\n",
        "\n",
        "# fit the model\n",
        "logreg100.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Zoto6U_d0lZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the scores on training and test set\n",
        "\n",
        "print('Training set score: {:.4f}'.format(logreg100.score(X_train, y_train)))\n",
        "\n",
        "print('Test set score: {:.4f}'.format(logreg100.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "kyYplDD40rCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that, C=100 results in higher test set accuracy and also a slightly increased training set accuracy. So, we can conclude that a more complex model should perform better.\n",
        "\n",
        "Now, I will investigate, what happens if we use more regularized model than the default value of C=1, by setting C=0.01."
      ],
      "metadata": {
        "id": "EzH6SgJh0x09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the Logsitic Regression model with C=001\n",
        "\n",
        "# instantiate the model\n",
        "logreg001 = LogisticRegression(C=0.01, solver='liblinear', random_state=0)\n",
        "\n",
        "\n",
        "# fit the model\n",
        "logreg001.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "WJgx0WWg0rE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the scores on training and test set\n",
        "\n",
        "print('Training set score: {:.4f}'.format(logreg001.score(X_train, y_train)))\n",
        "\n",
        "print('Test set score: {:.4f}'.format(logreg001.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "LTBMke3g0ycW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, if we use more regularized model by setting C=0.01, then both the training and test set accuracy decrease relatiev to the default parameters."
      ],
      "metadata": {
        "id": "B6wni18O04KI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare model accuracy with null accuracy:\n",
        "\n",
        "So, the model accuracy is 0.8501. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\n",
        "\n",
        "So, we should first check the class distribution in the test set."
      ],
      "metadata": {
        "id": "lL9gA9KB05yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check class distribution in test set\n",
        "\n",
        "y_test.value_counts()"
      ],
      "metadata": {
        "id": "wLNpatKD0yfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the occurences of most frequent class is 22067. So, we can calculate null accuracy by dividing 22067 by total number of occurences."
      ],
      "metadata": {
        "id": "I1lLdNKr09i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check null accuracy score\n",
        "\n",
        "null_accuracy = (22067/(22067+6372))\n",
        "\n",
        "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
      ],
      "metadata": {
        "id": "UVfVtcEq0yis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our model accuracy score is 0.8501 but null accuracy score is 0.7759. So, we can conclude that our Logistic Regression model is doing a very good job in predicting the class labels.\n",
        "\n",
        "Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n",
        "\n",
        "But, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\n",
        "\n",
        "We have another tool called Confusion matrix that comes to our rescue."
      ],
      "metadata": {
        "id": "lISO35ko1DGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix"
      ],
      "metadata": {
        "id": "9lwazgkk1FIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n",
        "\n",
        "Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n",
        "\n",
        "True Positives (TP) – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n",
        "\n",
        "True Negatives (TN) – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n",
        "\n",
        "False Positives (FP) – False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called Type I error.\n",
        "\n",
        "False Negatives (FN) – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called Type II error.\n",
        "\n",
        "These four outcomes are summarized in a confusion matrix given below."
      ],
      "metadata": {
        "id": "ePTZXriS1Joq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the Confusion Matrix and slice it into four pieces\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm)\n",
        "\n",
        "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
        "\n",
        "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
        "\n",
        "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
        "\n",
        "print('\\nFalse Negatives(FN) = ', cm[1,0])"
      ],
      "metadata": {
        "id": "vwWzjrpU1DBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix shows 20892 + 3285 = 24177 correct predictions and 3087 + 1175 = 4262 incorrect predictions.\n",
        "\n",
        "In this case, we have\n",
        "\n",
        "True Positives (Actual Positive:1 and Predict Positive:1) - 20892\n",
        "True Negatives (Actual Negative:0 and Predict Negative:0) - 3285\n",
        "False Positives (Actual Negative:0 but Predict Positive:1) - 1175 (Type I error)\n",
        "False Negatives (Actual Positive:1 but Predict Negative:0) - 3087 (Type II error)"
      ],
      "metadata": {
        "id": "c1ckjJKw1Pms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize confusion matrix with seaborn heatmap\n",
        "\n",
        "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'],\n",
        "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
        "\n",
        "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
      ],
      "metadata": {
        "id": "_aWN_Z-w0rHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Report:\n",
        "\n",
        "Classification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. I have described these terms in later.\n",
        "\n",
        "We can print a classification report as follows:-"
      ],
      "metadata": {
        "id": "aEwlev1A1kHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "XGsscbFo1Can"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification accuracy\n",
        "\n",
        "TP = cm[0,0]\n",
        "TN = cm[1,1]\n",
        "FP = cm[0,1]\n",
        "FN = cm[1,0]"
      ],
      "metadata": {
        "id": "ISu28gUE1Ce3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print classification accuracy\n",
        "\n",
        "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
        "\n",
        "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))"
      ],
      "metadata": {
        "id": "n76pSq-51Cjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification error\n",
        "\n",
        "\n",
        "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
        "\n",
        "print('Classification error : {0:0.4f}'.format(classification_error))"
      ],
      "metadata": {
        "id": "tq0U8XB9iEqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision:\n",
        "\n",
        "Precision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP).\n",
        "\n",
        "So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n",
        "\n",
        "Mathematically, precision can be defined as the ratio of TP to (TP + FP)."
      ],
      "metadata": {
        "id": "yx0qOPML11NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print precision score\n",
        "\n",
        "precision = TP / float(TP + FP)\n",
        "\n",
        "\n",
        "print('Precision : {0:0.4f}'.format(precision))"
      ],
      "metadata": {
        "id": "24NtHq8g1vYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall:\n",
        "\n",
        "Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall is also called Sensitivity.\n",
        "\n",
        "Recall identifies the proportion of correctly predicted actual positives.\n",
        "\n",
        "Mathematically, recall can be given as the ratio of TP to (TP + FN)."
      ],
      "metadata": {
        "id": "RLxqx_xq1410"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recall = TP / float(TP + FN)\n",
        "\n",
        "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
      ],
      "metadata": {
        "id": "IvUBNurx1va8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "True Positive Rate:\n",
        "\n",
        "True Positive Rate is synonymous with Recall."
      ],
      "metadata": {
        "id": "WHTowGiP1_wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_positive_rate = TP / float(TP + FN)\n",
        "\n",
        "\n",
        "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))"
      ],
      "metadata": {
        "id": "yNaMqU-z1veD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# False Positive Rate\n",
        "\n",
        "false_positive_rate = FP / float(FP + TN)\n",
        "\n",
        "\n",
        "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
      ],
      "metadata": {
        "id": "upSUV8t-18Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specificity\n",
        "\n",
        "specificity = TN / (TN + FP)\n",
        "\n",
        "print('Specificity : {0:0.4f}'.format(specificity))"
      ],
      "metadata": {
        "id": "KwkCloC12CT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f1-score:\n",
        "\n",
        "f1-score is the weighted harmonic mean of precision and recall. The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy."
      ],
      "metadata": {
        "id": "oMkUON_x2J3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Support:\n",
        "\n",
        "# Support is the actual number of occurrences of the class in our dataset."
      ],
      "metadata": {
        "id": "9xLEmEId2CYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjusting the threshold level"
      ],
      "metadata": {
        "id": "RBc7SF8N2Tw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 10 predicted probabilities of two classes- 0 and 1\n",
        "\n",
        "y_pred_prob = logreg.predict_proba(X_test)[0:10]\n",
        "\n",
        "y_pred_prob"
      ],
      "metadata": {
        "id": "KOQSUQR82CbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations¶\n",
        "In each row, the numbers sum to 1.\n",
        "There are 2 columns which correspond to 2 classes - 0 and 1.\n",
        "\n",
        "Class 0 - predicted probability that there is no rain tomorrow.\n",
        "\n",
        "Class 1 - predicted probability that there is rain tomorrow.\n",
        "\n",
        "Importance of predicted probabilities\n",
        "\n",
        "We can rank the observations by probability of rain or no rain.\n",
        "predict_proba process\n",
        "\n",
        "Predicts the probabilities\n",
        "\n",
        "Choose the class with the highest probability\n",
        "\n",
        "Classification threshold level\n",
        "\n",
        "There is a classification threshold level of 0.5.\n",
        "\n",
        "Class 1 - probability of rain is predicted if probability > 0.5.\n",
        "\n",
        "Class 0 - probability of no rain is predicted if probability < 0.5."
      ],
      "metadata": {
        "id": "2KCwQ7aI2X7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store the probabilities in dataframe\n",
        "\n",
        "y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - No rain tomorrow (0)', 'Prob of - Rain tomorrow (1)'])\n",
        "\n",
        "y_pred_prob_df"
      ],
      "metadata": {
        "id": "cvAqx7s92TWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 10 predicted probabilities for class 1 - Probability of rain\n",
        "\n",
        "logreg.predict_proba(X_test)[0:10, 1]"
      ],
      "metadata": {
        "id": "EETh6UPC2XhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the predicted probabilities for class 1 - Probability of rain\n",
        "\n",
        "y_pred1 = logreg.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "1WmLtKsZ2XkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot histogram of predicted probabilities\n",
        "\n",
        "\n",
        "# adjust the font size\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "\n",
        "# plot histogram with 10 bins\n",
        "plt.hist(y_pred1, bins = 10)\n",
        "\n",
        "\n",
        "# set the title of predicted probabilities\n",
        "plt.title('Histogram of predicted probabilities of rain')\n",
        "\n",
        "\n",
        "# set the x-axis limit\n",
        "plt.xlim(0,1)\n",
        "\n",
        "\n",
        "# set the title\n",
        "plt.xlabel('Predicted probabilities of rain')\n",
        "plt.ylabel('Frequency')"
      ],
      "metadata": {
        "id": "lnA1qaAw2TZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "We can see that the above histogram is highly positive skewed.\n",
        "\n",
        "\n",
        "The first column tell us that there are approximately 15000 observations with probability between 0.0 and 0.1.\n",
        "\n",
        "\n",
        "There are small number of observations with probability > 0.5.\n",
        "\n",
        "\n",
        "So, these small number of observations predict that there will be rain tomorrow.\n",
        "\n",
        "\n",
        "Majority of observations predict that there will be no rain tomorrow."
      ],
      "metadata": {
        "id": "7fmtTJNr2jW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import binarize\n",
        "\n",
        "for i in range(1,5):\n",
        "\n",
        "    cm1=0\n",
        "\n",
        "    y_pred1 = logreg.predict_proba(X_test)[:,1]\n",
        "\n",
        "    y_pred1 = y_pred1.reshape(-1,1)\n",
        "\n",
        "    y_pred2 = binarize(y_pred1, threshold=i/10)\n",
        "\n",
        "    y_pred2 = np.where(y_pred2 == 1, 'Yes', 'No')\n",
        "\n",
        "    cm1 = confusion_matrix(y_test, y_pred2)\n",
        "\n",
        "    print ('With',i/10,'threshold the Confusion Matrix is ','\\n\\n',cm1,'\\n\\n',\n",
        "\n",
        "            'with',cm1[0,0]+cm1[1,1],'correct predictions, ', '\\n\\n',\n",
        "\n",
        "            cm1[0,1],'Type I errors( False Positives), ','\\n\\n',\n",
        "\n",
        "            cm1[1,0],'Type II errors( False Negatives), ','\\n\\n',\n",
        "\n",
        "           'Accuracy score: ', (accuracy_score(y_test, y_pred2)), '\\n\\n',\n",
        "\n",
        "           'Sensitivity: ',cm1[1,1]/(float(cm1[1,1]+cm1[1,0])), '\\n\\n',\n",
        "\n",
        "           'Specificity: ',cm1[0,0]/(float(cm1[0,0]+cm1[0,1])),'\\n\\n',\n",
        "\n",
        "            '====================================================', '\\n\\n')"
      ],
      "metadata": {
        "id": "maw3shWK2g2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comments:\n",
        "\n",
        "In binary problems, the threshold of 0.5 is used by default to convert predicted probabilities into class predictions.\n",
        "\n",
        "\n",
        "Threshold can be adjusted to increase sensitivity or specificity.\n",
        "\n",
        "\n",
        "Sensitivity and specificity have an inverse relationship. Increasing one would always decrease the other and vice versa.\n",
        "\n",
        "\n",
        "We can see that increasing the threshold level results in increased accuracy.\n",
        "\n",
        "\n",
        "Adjusting the threshold level should be one of the last step you do in the model-building process."
      ],
      "metadata": {
        "id": "6LaquVK_261P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROC - AUC"
      ],
      "metadata": {
        "id": "Ses-WMoY2-sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Another tool to measure the classification model performance visually is ROC Curve. ROC Curve stands for Receiver Operating Characteristic Curve. An ROC Curve is a plot which shows the performance of a classification model at various classification threshold levels.\n",
        "\n",
        "The ROC Curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold levels.\n",
        "\n",
        "True Positive Rate (TPR) is also called Recall. It is defined as the ratio of TP to (TP + FN).\n",
        "\n",
        "False Positive Rate (FPR) is defined as the ratio of FP to (FP + TN).\n",
        "\n",
        "In the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positve. It will increase both True Positives (TP) and False Positives (FP)."
      ],
      "metadata": {
        "id": "0NyTMWqm3A5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot ROC Curve\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred1, pos_label = 'Yes')\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "\n",
        "plt.plot(fpr, tpr, linewidth=2)\n",
        "\n",
        "plt.plot([0,1], [0,1], 'k--' )\n",
        "\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "plt.title('ROC curve for RainTomorrow classifier')\n",
        "\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KPGV_tqC2g5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context."
      ],
      "metadata": {
        "id": "uzecEl-Q3IGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC AUC stands for Receiver Operating Characteristic - Area Under Curve. It is a technique to compare classifier performance. In this technique, we measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.\n",
        "\n",
        "So, ROC AUC is the percentage of the ROC plot that is underneath the curve."
      ],
      "metadata": {
        "id": "4WjWOGLr3J_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute ROC AUC\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "ROC_AUC = roc_auc_score(y_test, y_pred1)\n",
        "\n",
        "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
      ],
      "metadata": {
        "id": "wWvTYQjY2g8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comments:\n",
        "\n",
        "ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\n",
        "\n",
        "ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it will rain tomorrow or not."
      ],
      "metadata": {
        "id": "XviP0CQU3NBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cross-validated ROC AUC\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "Cross_validated_ROC_AUC = cross_val_score(logreg, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
        "\n",
        "print('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))"
      ],
      "metadata": {
        "id": "nhAjp4p73HqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k-Fold Cross Validation\n",
        "\n",
        "# Applying 5-Fold Cross Validation\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(logreg, X_train, y_train, cv = 5, scoring='accuracy')\n",
        "\n",
        "print('Cross-validation scores:{}'.format(scores))"
      ],
      "metadata": {
        "id": "c1QYs2UH3Htb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can summarize the cross-validation accuracy by calculating its mean.\n",
        "# compute Average cross-validation score\n",
        "\n",
        "print('Average cross-validation score: {:.4f}'.format(scores.mean()))"
      ],
      "metadata": {
        "id": "Avft3qC1AqC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our, original model score is found to be 0.8476. The average cross-validation score is 0.8474. So, we can conclude that cross-validation does not result in performance improvement."
      ],
      "metadata": {
        "id": "-c3lg3QWA310"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Optimization using GridSearch CV"
      ],
      "metadata": {
        "id": "5mco9ec7A5XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "parameters = [{'penalty':['l1','l2']},\n",
        "              {'C':[1, 10, 100, 1000]}]\n",
        "\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(estimator = logreg,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 5,\n",
        "                           verbose=0)\n",
        "\n",
        "\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "JXneLPlxAqHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examine the best model\n",
        "\n",
        "# best score achieved during the GridSearchCV\n",
        "print('GridSearch CV best score : {:.4f}\\n\\n'.format(grid_search.best_score_))\n",
        "\n",
        "# print parameters that give the best results\n",
        "print('Parameters that give the best results :','\\n\\n', (grid_search.best_params_))\n",
        "\n",
        "# print estimator that was chosen by the GridSearch\n",
        "print('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))"
      ],
      "metadata": {
        "id": "w3moL8P8A3Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate GridSearch CV score on test set\n",
        "\n",
        "print('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "DMN0DtvJA3Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comments:\n",
        "\n",
        "Our original model test accuracy is 0.8501 while GridSearch CV accuracy is 0.8539.\n",
        "We can see that GridSearch CV improve the performance for this particular model."
      ],
      "metadata": {
        "id": "Md3WrAP8BGJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results and conclusion"
      ],
      "metadata": {
        "id": "vYr-0rnPBJsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model accuracy score is 0.8501. So, the model does a very good job in predicting whether or not it will rain tomorrow in Australia.\n",
        "\n",
        "Small number of observations predict that there will be rain tomorrow. Majority of observations predict that there will be no rain tomorrow.\n",
        "\n",
        "The model shows no signs of overfitting.\n",
        "\n",
        "Increasing the value of C results in higher test set accuracy and also a slightly increased training set accuracy. So, we can conclude that a more complex model should perform better.\n",
        "\n",
        "Increasing the threshold level results in increased accuracy.\n",
        "\n",
        "ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it will rain tomorrow or not.\n",
        "\n",
        "Our original model accuracy score is 0.8501 whereas accuracy score after RFECV is 0.8500. So, we can obtain approximately similar accuracy but with reduced set of features.\n",
        "\n",
        "In the original model, we have FP = 1175 whereas FP1 = 1174. So, we get approximately same number of false positives. Also, FN = 3087 whereas FN1 = 3091. So, we get slighly higher false negatives.\n",
        "\n",
        "Our, original model score is found to be 0.8476. The average cross-validation score is 0.8474. So, we can conclude that cross-validation does not result in performance improvement.\n",
        "\n",
        "Our original model test accuracy is 0.8501 while GridSearch CV accuracy is 0.8507. We can see that GridSearch CV improve the performance for this particular model."
      ],
      "metadata": {
        "id": "9ISURuTgBNGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References:  \n",
        "\n",
        "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
        "\n",
        "https://en.wikipedia.org/wiki/Sigmoid_function\n",
        "\n",
        "https://www.statisticssolutions.com/assumptions-of-logistic-regression/\n",
        "\n",
        "https://en.wikipedia.org/wiki/Logistic_regression"
      ],
      "metadata": {
        "id": "GDSj8gp8BS63"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xL6bwzWuBEGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fGL219DBEKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nchmu_62g_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}